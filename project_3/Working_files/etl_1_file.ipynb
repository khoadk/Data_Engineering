{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL FOR 1 TESTING FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import configparser\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the params of the created redshift cluster \n",
    "\n",
    "- We need:\n",
    "    - The redshift cluster <font color='red'>endpoint</font>\n",
    "    - The <font color='red'>IAM role ARN</font> that give access to Redshift to read from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open(os.getcwd()+'/test_dwh.cfg'))\n",
    "\n",
    "KEY=config.get('AWS','key')\n",
    "SECRET= config.get('AWS','secret')\n",
    "\n",
    "DWH_DB= config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER= config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD= config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT = config.get(\"DWH\",\"DWH_PORT\")\n",
    "\n",
    "DWH_ENDPOINT = config.get(\"DWH\",\"DWH_ENDPOINT\")\n",
    "DWH_ROLE_ARN = config.get(\"DWH\",\"DWH_ROLE_ARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connect to the Redshift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: dwhuser@dwh'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema for Song Play Analysis:\n",
    "Using the song and event datasets, you'll need to create a star schema optimized for queries on song play analysis. This includes the following tables.\n",
    "\n",
    "**The primary benefit of a star schema is its simplicity for users to write, and databases to process: queries are written with simple inner joins between the facts and a small number of dimensions.**\n",
    "\n",
    "### Fact Table\n",
    "- **songplays** - records in log data associated with song plays i.e. records with page NextSong: songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent. Note: Upsert on level attribure.\n",
    "\n",
    "### Dimension Tables\n",
    "- **users** users in the app: user_id, first_name, last_name, gender, level. Note: Upsert on level attribure.\n",
    "- **songs** songs in music database: song_id, title, artist_id, year, duration\n",
    "- **artists** artists in music database: artist_id, name, location, latitude, longitude\n",
    "- **time** timestamps of records in songplays broken down into specific units: start_time, hour, day, week, month, year, weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Staging tables:\n",
    "- Load data from S3 to staging tables on Redshift.\n",
    "- There are 2 staging tables (more detail from  first look datasets):\n",
    "    + song dataset.\n",
    "    + log dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Song Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cl33boinwx8d.us-east-1.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "CREATE SCHEMA IF NOT EXISTS staging;\n",
    "SET search_path TO staging;\n",
    "\n",
    "DROP TABLE IF EXISTS song_dataset;\n",
    "DROP TABLE IF EXISTS log_dataset;\n",
    "DROP TABLE IF EXISTS raw_fact_table;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS song_dataset \n",
    "(\n",
    "    artist_id varchar,\n",
    "    artist_latitude varchar,\n",
    "    artist_location varchar,\n",
    "    artist_longitude varchar,\n",
    "    artist_name varchar,\n",
    "    duration varchar,\n",
    "    num_songs varchar,\n",
    "    song_id varchar,\n",
    "    title varchar,\n",
    "    year varchar\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS log_dataset \n",
    "(\n",
    "    artist varchar,\n",
    "    auth varchar,\n",
    "    firstname varchar,\n",
    "    gender varchar,\n",
    "    iteminsession varchar,\n",
    "    lastname varchar,\n",
    "    length varchar,\n",
    "    level varchar,\n",
    "    location varchar,\n",
    "    method varchar,\n",
    "    page varchar,\n",
    "    registration varchar,\n",
    "    sessionid varchar,\n",
    "    song varchar,\n",
    "    status varchar,\n",
    "    ts varchar,\n",
    "    useragent varchar,\n",
    "    userid varchar\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS raw_fact_table \n",
    "(\n",
    "    artist varchar,\n",
    "    auth varchar,\n",
    "    firstname varchar,\n",
    "    gender varchar,\n",
    "    iteminsession varchar,\n",
    "    lastname varchar,\n",
    "    length varchar,\n",
    "    level varchar,\n",
    "    location varchar,\n",
    "    method varchar,\n",
    "    page varchar,\n",
    "    registration varchar,\n",
    "    sessionid varchar,\n",
    "    song varchar,\n",
    "    status varchar,\n",
    "    ts varchar,\n",
    "    useragent varchar,\n",
    "    userid varchar,\n",
    "    artist_id varchar,\n",
    "    song_id varchar\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Loading single json file into Song Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cl33boinwx8d.us-east-1.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "CPU times: user 1.36 ms, sys: 3.47 ms, total: 4.83 ms\n",
      "Wall time: 4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qry = \"\"\"\n",
    "    copy song_dataset from 's3://udacity-dend/song_data/A/A/A/TRAAAAK128F9318786.json'\n",
    "    credentials 'aws_iam_role={}'\n",
    "    json 'auto'\n",
    "    region 'us-west-2';\n",
    "\"\"\".format(DWH_ROLE_ARN)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Loading single json file into Log Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cl33boinwx8d.us-east-1.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "CPU times: user 4.94 ms, sys: 173 Âµs, total: 5.11 ms\n",
      "Wall time: 4.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qry = \"\"\"\n",
    "    copy log_dataset from 's3://udacity-dend/log_data/2018/11/2018-11-01-events.json'\n",
    "    credentials 'aws_iam_role={}'\n",
    "    json 's3://udacity-dend/log_json_path.json'\n",
    "    region 'us-west-2';\n",
    "\"\"\".format(DWH_ROLE_ARN)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Analytic tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cl33boinwx8d.us-east-1.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "CREATE SCHEMA IF NOT EXISTS analyst_nodist;\n",
    "SET search_path TO analyst_nodist;\n",
    "\n",
    "DROP TABLE IF EXISTS users;\n",
    "DROP TABLE IF EXISTS songs;\n",
    "DROP TABLE IF EXISTS artists;\n",
    "DROP TABLE IF EXISTS time;\n",
    "DROP TABLE IF EXISTS songplays;\n",
    "\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS users \n",
    "(\n",
    "    user_id varchar,\n",
    "    first_name varchar,\n",
    "    last_name varchar,\n",
    "    gender varchar,\n",
    "    level varchar,\n",
    "    PRIMARY KEY (user_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS songs (\n",
    "    song_id varchar, \n",
    "    title varchar, \n",
    "    artist_id varchar, \n",
    "    year int, \n",
    "    duration real, \n",
    "    PRIMARY KEY (song_id) \n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS artists (\n",
    "    artist_id varchar, \n",
    "    name varchar, \n",
    "    location varchar, \n",
    "    latitude real, \n",
    "    longitude real, \n",
    "    PRIMARY KEY (artist_id) \n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS time (\n",
    "    start_time varchar, \n",
    "    hour varchar, \n",
    "    day varchar, -- Day of month\n",
    "    week varchar, -- Week of year\n",
    "    month varchar, \n",
    "    year varchar, \n",
    "    weekday varchar, \n",
    "    PRIMARY KEY (start_time) \n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS songplays (\n",
    "    songplay_id bigint IDENTITY(0,1), \n",
    "    start_time varchar, \n",
    "    user_id varchar, \n",
    "    level varchar, \n",
    "    song_id varchar, \n",
    "    artist_id varchar, \n",
    "    session_id varchar, \n",
    "    location varchar, \n",
    "    user_agent varchar,\n",
    "    PRIMARY KEY (songplay_id) \n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ETL from staging tables into analytic tables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 Users table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cl33boinwx8d.us-east-1.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "15 rows affected.\n",
      "Done.\n",
      "Done.\n",
      "2 rows affected.\n",
      "12 rows affected.\n",
      "2 rows affected.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "BEGIN;\n",
    "\n",
    "--  insert users table having duplicate row\n",
    "INSERT into analyst_nodist.users (user_id, first_name, last_name, gender, level)\n",
    "SELECT userId, firstName, lastName, gender, level\n",
    "FROM staging.log_dataset\n",
    "WHERE userId IS NOT NULL;\n",
    "\n",
    "-- Star remove the duplicate rows\n",
    "-- First identify the rows that are duplicate\n",
    "CREATE TEMP TABLE duplicate_user_id AS\n",
    "SELECT user_id\n",
    "FROM analyst_nodist.users\n",
    "GROUP BY user_id\n",
    "HAVING COUNT(*) > 1;\n",
    "\n",
    "-- Extract one copy of all the duplicate rows\n",
    "CREATE TEMP TABLE new_users(LIKE analyst_nodist.users);\n",
    "\n",
    "INSERT into new_users\n",
    "SELECT DISTINCT *\n",
    "FROM analyst_nodist.users\n",
    "WHERE user_id IN(\n",
    "     SELECT user_id\n",
    "     FROM duplicate_user_id\n",
    ");\n",
    "\n",
    "-- Remove rows that were duplicated (all copies).\n",
    "DELETE FROM analyst_nodist.users\n",
    "WHERE user_id IN(\n",
    "     SELECT user_id\n",
    "     FROM duplicate_user_id\n",
    ");\n",
    "\n",
    "-- Insert back into the single copies\n",
    "INSERT into analyst_nodist.users\n",
    "SELECT *\n",
    "FROM new_users;\n",
    "\n",
    "-- Cleanup\n",
    "DROP TABLE duplicate_user_id;\n",
    "DROP TABLE new_users;\n",
    "COMMIT;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3..3.2 songs table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cl33boinwx8d.us-east-1.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "1 rows affected.\n",
      "Done.\n",
      "Done.\n",
      "0 rows affected.\n",
      "0 rows affected.\n",
      "0 rows affected.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "BEGIN;\n",
    "\n",
    "--  insert songs table MAY have duplicate row\n",
    "INSERT into analyst_nodist.songs (song_id, title, artist_id, year, duration) \n",
    "SELECT song_id, title, artist_id, convert(integer, year), convert(real, duration)\n",
    "FROM staging.song_dataset;\n",
    "\n",
    "-- Star remove the duplicate rows\n",
    "-- First identify the rows that are duplicate\n",
    "CREATE TEMP TABLE duplicate_song_id AS\n",
    "SELECT song_id\n",
    "FROM analyst_nodist.songs\n",
    "GROUP BY song_id\n",
    "HAVING COUNT(*) > 1;\n",
    "\n",
    "-- Extract one copy of all the duplicate rows\n",
    "CREATE TEMP TABLE new_songs(LIKE analyst_nodist.songs);\n",
    "\n",
    "INSERT into new_songs\n",
    "SELECT DISTINCT *\n",
    "FROM analyst_nodist.songs\n",
    "WHERE song_id IN(\n",
    "     SELECT song_id\n",
    "     FROM duplicate_song_id\n",
    ");\n",
    "\n",
    "-- Remove rows that were duplicated (all copies).\n",
    "DELETE FROM analyst_nodist.songs\n",
    "WHERE song_id IN(\n",
    "     SELECT song_id\n",
    "     FROM duplicate_song_id\n",
    ");\n",
    "\n",
    "-- Insert back into the single copies\n",
    "INSERT into analyst_nodist.songs\n",
    "SELECT *\n",
    "FROM new_songs;\n",
    "\n",
    "-- Cleanup\n",
    "DROP TABLE duplicate_song_id;\n",
    "DROP TABLE new_songs;\n",
    "\n",
    "COMMIT;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Artists table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "BEGIN;\n",
    "\n",
    "--  insert artists table MAY have duplicate row\n",
    "INSERT into analyst_nodist.artists (artist_id, name, location, latitude, longitude) \n",
    "SELECT artist_id, artist_name, artist_location, convert(real, artist_latitude), convert(real, artist_longitude)\n",
    "FROM staging.song_dataset;\n",
    "\n",
    "-- Star remove the duplicate rows\n",
    "-- First identify the rows that are duplicate\n",
    "CREATE TEMP TABLE duplicate_artist_id AS\n",
    "SELECT artist_id\n",
    "FROM analyst_nodist.artists\n",
    "GROUP BY artist_id\n",
    "HAVING COUNT(*) > 1;\n",
    "\n",
    "-- Extract one copy of all the duplicate rows\n",
    "CREATE TEMP TABLE new_artists(LIKE analyst_nodist.artists);\n",
    "\n",
    "INSERT into new_artists\n",
    "SELECT DISTINCT *\n",
    "FROM analyst_nodist.songs\n",
    "WHERE artist_id IN(\n",
    "     SELECT artist_id\n",
    "     FROM duplicate_artist_id\n",
    ");\n",
    "\n",
    "-- Remove rows that were duplicated (all copies).\n",
    "DELETE FROM analyst_nodist.artists\n",
    "WHERE artist_id IN(\n",
    "     SELECT artist_id\n",
    "     FROM duplicate_artist_id\n",
    ");\n",
    "\n",
    "-- Insert back into the single copies\n",
    "INSERT into analyst_nodist.artists\n",
    "SELECT *\n",
    "FROM new_artists;\n",
    "\n",
    "-- Cleanup\n",
    "DROP TABLE duplicate_artist_id;\n",
    "DROP TABLE new_artists;\n",
    "\n",
    "COMMIT;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 Time table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cl33boinwx8d.us-east-1.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "225 rows affected.\n",
      "Done.\n",
      "Done.\n",
      "42 rows affected.\n",
      "450 rows affected.\n",
      "42 rows affected.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "BEGIN;\n",
    "\n",
    "-- Create TEMP TABLE temp_timestamp to convert ts epoch to timestamp\n",
    "DROP TABLE IF EXISTS temp_timestamp;\n",
    "\n",
    "CREATE TEMP TABLE temp_timestamp AS\n",
    "SELECT timestamp with time zone 'epoch' + convert(BIGINT, ts)/1000 * interval '1 second' AS timestamp\n",
    "FROM staging.log_dataset;\n",
    "\n",
    "--  insert artists table MAY have duplicate row\n",
    "INSERT into analyst_nodist.time (start_time, hour, day, week, month, year, weekday) \n",
    "SELECT stag.ts,\n",
    "to_char (temp.timestamp, 'HH24'),\n",
    "to_char (temp.timestamp, 'DD'),\n",
    "to_char (temp.timestamp, 'WW'),\n",
    "to_char (temp.timestamp, 'MM'),\n",
    "to_char (temp.timestamp, 'YYYY'),\n",
    "to_char (temp.timestamp, 'Day')\n",
    "FROM staging.log_dataset stag, temp_timestamp temp;\n",
    "\n",
    "-- Star remove the duplicate rows\n",
    "-- First identify the rows that are duplicate\n",
    "CREATE TEMP TABLE duplicate_start_time AS\n",
    "SELECT start_time\n",
    "FROM analyst_nodist.time\n",
    "GROUP BY start_time\n",
    "HAVING COUNT(*) > 1;\n",
    "\n",
    "-- Extract one copy of all the duplicate rows\n",
    "CREATE TEMP TABLE new_time(LIKE analyst_nodist.time);\n",
    "\n",
    "INSERT into new_time\n",
    "SELECT DISTINCT *\n",
    "FROM analyst_nodist.time\n",
    "WHERE start_time IN(\n",
    "     SELECT start_time\n",
    "     FROM duplicate_start_time\n",
    ");\n",
    "\n",
    "-- Remove rows that were duplicated (all copies).\n",
    "DELETE FROM analyst_nodist.time\n",
    "WHERE start_time IN(\n",
    "     SELECT start_time\n",
    "     FROM duplicate_start_time\n",
    ");\n",
    "\n",
    "-- Insert back into the single copies\n",
    "INSERT into analyst_nodist.time\n",
    "SELECT *\n",
    "FROM new_time;\n",
    "\n",
    "-- Cleanup\n",
    "DROP TABLE duplicate_start_time;\n",
    "DROP TABLE new_time;\n",
    "DROP TABLE temp_timestamp;\n",
    "\n",
    "COMMIT;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.5 songplays table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cl33boinwx8d.us-east-1.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "11 rows affected.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "BEGIN;\n",
    "\n",
    "INSERT into songplays( \n",
    "    start_time, user_id, level,\n",
    "    song_id, artist_id,\n",
    "    session_id, location, user_agent)\n",
    "SELECT log_dataset.ts, log_dataset.userId, log_dataset.level,\n",
    "song_dataset.song_id, song_dataset.artist_id,\n",
    "log_dataset.sessionId, log_dataset.location, log_dataset.userAgent\n",
    "FROM staging.log_dataset log_dataset LEFT JOIN staging.song_dataset song_dataset\n",
    "ON log_dataset.artist = song_dataset.artist_name\n",
    "AND log_dataset.song =  song_dataset.title\n",
    "AND log_dataset.length =  song_dataset.duration\n",
    "WHERE log_dataset.page = 'NextSong';\n",
    "\n",
    "COMMIT;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Automate  the copying\n",
    "\n",
    "- Make sure that the `DWH_ROLE_ARN` is substituted with the correct value in each query\n",
    "- Perform the data loading twice once for each schema (dist and nodist)\n",
    "- Collect timing statistics to compare the insertion times\n",
    "\n",
    "We have scripted the insertion as found below in the function `loadTables` which\n",
    "returns a pandas dataframe containing timing statistics for the copy operations\n",
    "\n",
    "```sql\n",
    "copy song_dataset from 's3://udacity-dend/song_data' \n",
    "credentials 'aws_iam_role=<DWH_ROLE_ARN>'\n",
    "json 'auto'\n",
    "gzip region 'us-west-2';\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTables(schema, tables):\n",
    "    loadTimes = []\n",
    "    SQL_SET_SCEMA = \"SET search_path TO {};\".format(schema)\n",
    "    %sql $SQL_SET_SCEMA\n",
    "    \n",
    "    for table in tables:\n",
    "        SQL_COPY = \"\"\"\n",
    "copy {} from 's3://udacity-dend/song_data/A/A/A/TRAAAAK128F9318786.json'\n",
    "credentials 'aws_iam_role={}'\n",
    "JSON 'auto'\n",
    "region 'us-west-2';\n",
    "        \"\"\".format(table, DWH_ROLE_ARN)\n",
    "\n",
    "        print(\"======= LOADING TABLE: ** {} ** IN SCHEMA ==> {} =======\".format(table, schema))\n",
    "        print(SQL_COPY)\n",
    "\n",
    "        t0 = time()\n",
    "        %sql $SQL_COPY\n",
    "        loadTime = time()-t0\n",
    "        loadTimes.append(loadTime)\n",
    "\n",
    "        print(\"=== DONE IN: {0:.2f} sec\\n\".format(loadTime))\n",
    "    return pd.DataFrame({\"table\":tables, \"loadtime_\"+schema:loadTimes}).set_index('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- List of the tables to be loaded\n",
    "tables = [\"song_dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cl33boinwx8d.us-east-1.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "======= LOADING TABLE: ** song_dataset ** IN SCHEMA ==> staging =======\n",
      "\n",
      "copy song_dataset from 's3://udacity-dend/song_data/A/A/A/TRAAAAK128F9318786.json'\n",
      "credentials 'aws_iam_role=arn:aws:iam::668674852354:role/dwhRole'\n",
      "JSON 'auto'\n",
      "region 'us-west-2';\n",
      "        \n",
      " * postgresql://dwhuser:***@dwhcluster.cl33boinwx8d.us-east-1.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "=== DONE IN: 14.11 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "staging = loadTables(\"staging\", tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
